{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2P Lending Dataset Preparation\n",
    "\n",
    "##### Luis Eduardo Boiko Ferreira,  PPGIa - PUCPR, luiseduardo.boiko@ppgia.pucpr.br\n",
    "##### Jean Paul Barddal,  PPGIa - PUCPR, jean.barddal@ppgia.pucpr.br\n",
    "##### Heitor Murilo Gomes, INFRES - Institut Mines-Télécom, heitor.gomes@telecom-paristech.fr\n",
    "##### Fabrício Enembreck, PPGIa - PUCPR, fabricio@ppgia.pucpr.br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script has the goal of merging the data made available from [Lending Club](https://www.lendingclub.com) between 2007 and 2016.\n",
    "In this work, we tackle only \"Charged Off\" and \"Fully Paid\" loans.\n",
    "The main steps taken to prepare the dataset are the following:\n",
    "\n",
    "1. Data load and header sanity check\n",
    "2. Data filter (charged off and fully paid) and concatenation\n",
    "3. Removal of features to avoid data leakage\n",
    "4. Removal and treatment of string variables\n",
    "5. Removal of instances (loan requests) with many missing values\n",
    "6. Removal of features (attributes) with many missing values\n",
    "7. Removal of variables of low variability\n",
    "8. Missing values imputation\n",
    "\n",
    "It is also important to mention that this script prepares two different versions of the dataset.\n",
    "The first is outputted between steps #4 and #5, and the other after step #8.\n",
    "The idea is to verify if the pre-processing that takes place between steps #5 and #8 impact somehow the learning algorithms and sampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data load and header sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Loads all datasets\n",
    "df2007to2011 = pd.read_csv(\"./LoanStats3a_securev1.csv\", low_memory=False, skiprows=[0])\n",
    "df2012to2013 = pd.read_csv(\"./LoanStats3b_securev1.csv\", low_memory=False, skiprows=[0])\n",
    "df2014       = pd.read_csv(\"./LoanStats3c_securev1.csv\", low_memory=False, skiprows=[0])\n",
    "df2015       = pd.read_csv(\"./LoanStats3d_securev1.csv\", low_memory=False, skiprows=[0])\n",
    "df2016Q1     = pd.read_csv(\"./LoanStats_securev1_2016Q1.csv\", low_memory=False, skiprows=[0])\n",
    "df2016Q2     = pd.read_csv(\"./LoanStats_securev1_2016Q2.csv\", low_memory=False, skiprows=[0])\n",
    "df2016Q3     = pd.read_csv(\"./LoanStats_securev1_2016Q3.csv\", low_memory=False, skiprows=[0])\n",
    "df2016Q4     = pd.read_csv(\"./LoanStats_securev1_2016Q4.csv\", low_memory=False, skiprows=[0])\n",
    "\n",
    "all_dfs = [df2007to2011, df2012to2013, df2014, df2015, df2016Q1, df2016Q2, df2016Q3, df2016Q4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking out how the data is shaped and if they match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columnsFirstDF = list(all_dfs[0].columns.values)\n",
    "error = False\n",
    "for df in all_dfs:\n",
    "    if set(df.columns.values) != set(columnsFirstDF):\n",
    "        error = True\n",
    "\n",
    "if error:\n",
    "    print(\"Subfiles are not maching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data filter (charged off and fully paid) and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578331, 128)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(all_dfs)\n",
    "#### Replaces 'loan_status' \"'Does not meet the credit policy. Status:Charged Off' 'Current'\" \n",
    "#### and \"'Does not meet the credit policy. Status:Fully Paid'\"\n",
    "df['loan_status'] = df['loan_status'].replace(\"'Does not meet the credit policy. Status:Charged Off' 'Current'\",\n",
    "                                              \"Charged Off\")\n",
    "df['loan_status'] = df['loan_status'].replace(\"'Does not meet the credit policy. Status:Fully Paid'\", \n",
    "                                              \"Fully Paid\")\n",
    "\n",
    "\n",
    "\n",
    "#### Filters dataset to contain only \"Charged Off\" and \"Fully Paid\" loans\n",
    "df = df.loc[(df.loan_status == \"Charged Off\") | (df.loan_status == \"Fully Paid\")]\n",
    "\n",
    "\n",
    "#### Converts the class to 0s and 1s\n",
    "df['loan_status'] = df['loan_status'].replace(\"Charged Off\", 1)\n",
    "df['loan_status'] = df['loan_status'].replace(\"Fully Paid\", 0)\n",
    "\n",
    "#### Converts this column to numbers, just in case.\n",
    "df['loan_status'] = pd.to_numeric(df['loan_status'], errors='ignore')\n",
    "\n",
    "df.set_index('id', inplace = True)\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "#### There are some missing values listed as a string \n",
    "#### in the 'n/a' format, so let's replace these for treatment later\n",
    "df.replace('n/a', np.nan, inplace = True)\n",
    "\n",
    "\n",
    "#### Sorts the dataset for the sake of visualization purposes \n",
    "df.sort_index(inplace = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Removal of features to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Here, we keep only the variables listed in the dictionary file\n",
    "\n",
    "featuresToKeep = ['acc_open_past_24mths','addr_state','annual_inc', \n",
    "                 'annual_inc_joint','application_type','avg_cur_bal', \n",
    "                 'bc_open_to_buy','bc_util','chargeoff_within_12_mths', \n",
    "                 'collections_12_mths_ex_med','delinq_2yrs','delinq_amnt',\n",
    "                 'dti','dti_joint','earliest_cr_line','emp_length', \n",
    "                 'fico_range_high','fico_range_low','grade','home_ownership', \n",
    "                 'initial_list_status','inq_last_6mths','installment',\n",
    "                 'int_rate', 'loan_amnt','mths_since_last_delinq', \n",
    "                 'mths_since_last_major_derog','mths_since_last_record', \n",
    "                 'num_accts_ever_120_pd','open_acc','pub_rec', \n",
    "                 'pub_rec_bankruptcies','revol_bal','revol_bal_joint', \n",
    "                 'revol_util','sec_app_chargeoff_within_12_mths', \n",
    "                 'sec_app_collections_12_mths_ex_med','sec_app_earliest_cr_line', \n",
    "                 'sec_app_fico_range_high','sec_app_fico_range_low', \n",
    "                 'sec_app_inq_last_6mths','sec_app_mort_acc', \n",
    "                 'sec_app_mths_since_last_major_derog', \n",
    "                 'sec_app_num_rev_accts','sec_app_open_acc', \n",
    "                 'sec_app_open_il_6m','sec_app_revol_util', \n",
    "                 'sub_grade','tax_liens','term','tot_hi_cred_lim', \n",
    "                 'total_acc','total_bal_ex_mort','total_bc_limit', \n",
    "                 'total_il_high_credit_limit','total_rev_hi_lim', \n",
    "                 'verification_status', 'loan_status']\n",
    "\n",
    "df = df[featuresToKeep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Removal and treatment of string variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up numeric data...\n",
      "Applying one hot encoding...\n",
      "Cleaning up date data...\n",
      "The new shape is now (578331, 161)\n"
     ]
    }
   ],
   "source": [
    "df_string = df.select_dtypes(exclude=[np.number])\n",
    "# print(df_string.shape)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "# display(df_string.head(1))\n",
    "\n",
    "print(\"Cleaning up numeric data...\")\n",
    "#### Converts some features to numeric\n",
    "def convertToNumeric(dataframe, list_of_attributes):\n",
    "    for f in list_of_attributes:\n",
    "        dataframe[f].replace(regex = True, inplace=True, to_replace=r'[^\\d.]+', value = r'')\n",
    "        dataframe[f] = pd.to_numeric(dataframe[f], errors='ignore')\n",
    "\n",
    "features_to_convert_to_numeric = ['term', \n",
    "                                  'revol_util', \n",
    "                                  'int_rate']\n",
    "convertToNumeric(df, features_to_convert_to_numeric)\n",
    "\n",
    "\n",
    "print(\"Applying one hot encoding...\")\n",
    "#### Applies one-hot-encoding to categorical variables\n",
    "def oneHotEncoding(dataframe, columnsToEncode):\n",
    "    new_dummies = []\n",
    "    for feature in columnsToEncode:\n",
    "        # creates dummies\n",
    "        dummies = pd.get_dummies(dataframe[feature], prefix=feature, prefix_sep='_')\n",
    "        for v in dummies.columns.values:\n",
    "            new_dummies.append(v)\n",
    "        # drops the feature\n",
    "        dataframe.drop(feature, axis = 1, inplace = True)\n",
    "        # appends n-1 features (the last is not necessary)\n",
    "        dummies.drop(dummies.columns[len(dummies.columns)-1], axis = 1, inplace=True)\n",
    "        dataframe = dataframe.join(dummies)\n",
    "    return dataframe, new_dummies\n",
    "\n",
    "categorical_features = ['grade', \n",
    "                        'sub_grade', \n",
    "                        'emp_length', \n",
    "                        'home_ownership', \n",
    "                        'verification_status', \n",
    "                        'addr_state', \n",
    "                        'initial_list_status', \n",
    "                        'application_type']\n",
    "df, new_dummies = oneHotEncoding(df, categorical_features)\n",
    "\n",
    "print(\"Cleaning up date data...\")\n",
    "#### TREATS DATE COLUMNS\n",
    "from datetime import datetime\n",
    "def separateDates(dataframe, columns):\n",
    "    for f in columns:\n",
    "        dataframe[f] = pd.to_datetime(dataframe[f], format='%b-%Y')\n",
    "        year = dataframe[f].apply(lambda x: x.strftime('%Y') if not pd.isnull(x) else '')\n",
    "        month = dataframe[f].apply(lambda x: x.strftime('%m') if not pd.isnull(x) else '')    \n",
    "        dataframe.drop(f, axis = 1, inplace = True)\n",
    "        df[(f + '_month')] = month\n",
    "        df[(f + '_year')] = year\n",
    "        df[(f + '_month')] = pd.to_numeric(df[(f + '_month')])\n",
    "        df[(f + '_year')] = pd.to_numeric(df[(f + '_year')])        \n",
    "    return df\n",
    "\n",
    "date_columns = ['earliest_cr_line']\n",
    "# all of these dates are in the mmm-YYYY format\n",
    "# and we wish to break them down into two separate columns: mm and YYYY\n",
    "df = separateDates(df, date_columns)\n",
    "\n",
    "print(\"The new shape is now {}\".format(df.shape))\n",
    "# display(df.head(1))\n",
    "# print(new_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's output the dataset now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./p2p_lendingclub.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Removal of instances (loan requests) with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578331, 161)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Getting rid of instances with too many missing values (above 90%)\n",
    "df.dropna(thresh = 0.5 * df.shape[1], axis = 0, inplace = True)\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Removal of features (attributes) with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578331, 143)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Removes all features with more than 50% of the values missing\n",
    "df.dropna(thresh = 0.5 * df.shape[0], axis = 1, inplace = True)\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Removal of variables of low variability (below 25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chargeoff_within_12_mths has a variability of 0.007214560546452864\n",
      "collections_12_mths_ex_med has a variability of 0.011065669447926973\n",
      "delinq_2yrs has a variability of 0.18065606028381676\n",
      "delinq_amnt has a variability of 0.0031279665105277132\n",
      "num_accts_ever_120_pd has a variability of 0.22768339043506824\n",
      "pub_rec has a variability of 0.15178677954320274\n",
      "pub_rec_bankruptcies has a variability of 0.11668634464037786\n",
      "tax_liens has a variability of 0.02426109992875569\n",
      "term has a variability of 0.23898252039057222\n",
      "(578331, 134)\n"
     ]
    }
   ],
   "source": [
    "toRemove = []\n",
    "for attribute in df.columns.values:\n",
    "    if attribute != 'loan_status' and attribute not in new_dummies:\n",
    "        count = pd.Series.value_counts(df[attribute])\n",
    "        maxCount = np.max(count)\n",
    "        variability = 1.0 - (float(maxCount) / count.sum())\n",
    "        if variability < .25:\n",
    "            print(\"{} has a variability of {}\".format(attribute, variability))            \n",
    "            toRemove.append(attribute)\n",
    "            \n",
    "            \n",
    "for f in toRemove:\n",
    "    df.drop(f, axis = 1, inplace = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Missing values imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578331, 134)\n"
     ]
    }
   ],
   "source": [
    "#### Numeric features are imputed with the median, \n",
    "#### while categorical features are imputed with the mode\n",
    "for f in df.columns.values:\n",
    "    if df[f].dtype == np.float64 or df[f].dtype == np.int64:\n",
    "        df[f].fillna(df[f].median(),inplace = True)\n",
    "    else:\n",
    "        df[f].fillna(df[f].value_counts().index[0], inplace = True)\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves this final DF to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./p2p_lendingclub_filtered.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
